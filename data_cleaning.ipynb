{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consumer Behaviour data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install pandas\n",
    "! pip3 install matplotlib.pyplot\n",
    "! pip3 install plotly.express\n",
    "! pip3 install openpyxl\n",
    "! pip3 install petl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import petl as etl\n",
    "import glob as glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import files from un-zipped folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csv files\n",
    "- Using glob, we extract all the files from the file ending in '.csv'\n",
    "- We use iteration to loop each file:\n",
    "    - export it into frame\n",
    "    - create a new 'store' column, as then alter the basename, removing the '.csv' and inserting file name into 'store' to differentiate which store each row belows too.\n",
    "    - we then append frame to the csv_data empty object\n",
    "- We use concatenation to bring all the files together\n",
    "- Finally exporting to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_data_import = glob.glob('data/*.csv')\n",
    "# csv_data = []\n",
    "# for csv in csv_data_import:\n",
    "#     frame = pd.read_csv(csv)\n",
    "#     frame['store'] = os.path.basename(csv).split(\".\")[0]\n",
    "#     csv_data.append(frame)\n",
    "\n",
    "# csv_final = pd.concat(csv_data, ignore_index=True)\n",
    "\n",
    "# csv_final.to_csv('index_output/csv_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### json files\n",
    "- Using glob, we extract all the files from the file ending in '.json'\n",
    "- We use iteration to loop each file:\n",
    "    - export it into frame\n",
    "    - create a new 'store' column, as then alter the basename, removing the '.json' and inserting file name into 'store' to differentiate which store each row belows too.\n",
    "    - we then append frame to the json_data empty object\n",
    "- We use concatenation to bring all the files together\n",
    "- Finally exporting to json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_data_import = glob.glob('data/*.json')\n",
    "# print(json_data_import)\n",
    "\n",
    "# json_data = []\n",
    "# for json in json_data_import:\n",
    "#     frame = pd.read_json(json)\n",
    "#     frame['store'] = os.path.basename(json).split(\".\")[0]\n",
    "#     json_data.append(frame)\n",
    "\n",
    "# json_final = pd.concat(json_data\n",
    "# json_final.to_csv('index_output/json_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import csv files into etl tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_table = etl.fromcsv('csv_final.csv')\n",
    "# json_table = etl.fromcsv('json_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csv table\n",
    "- As some of the columns have slightly differnet names but same meaningful data, We use the etl.addfield propety to create a 'new_quantity' and 'new_item\" \n",
    "column and added the values from the various named columns to it\n",
    "- With the new columns in place, we removed all the 'old' columns using etl.cutout\n",
    "- In order to match other tables and to succesffuly join them, we need the columns names the same.\n",
    "    - we did this by converting all cells within the 'store' column, spliting at the _ and replacing the other _ to a space. ('2010-2020_bradford_branch' --> 'bradford branch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_table =etl.addfield(csv_table, 'new_quantity', lambda cell: cell['quantity'] + cell ['total_quantity'] + cell ['quantity_purchased'] + cell ['total_quantity_purchased'])\n",
    "# csv_table =etl.addfield(csv_table, 'new_item', lambda cell: cell['item'] + cell ['product'] + cell ['sku'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# csv_table = etl.cutout(csv_table, 'quantity')\n",
    "# csv_table = etl.cutout(csv_table, 'total_quantity')\n",
    "# csv_table = etl.cutout(csv_table, 'quantity_purchased')\n",
    "# csv_table = etl.cutout(csv_table, 'total_quantity_purchased')\n",
    "# csv_table = etl.cutout(csv_table, 'item')\n",
    "# csv_table = etl.cutout(csv_table, 'product')\n",
    "# csv_table = etl.cutout(csv_table, 'sku')\n",
    "\n",
    "# csv_table = etl.convert(csv_table, 'store', lambda cell: \"_\".join(cell.split('_')[1:]).replace(\"_\", \" \").strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### json table\n",
    "- As some of the columns have slightly differnet names but same meaningful data, We use the etl.addfield propety to create a 'new_quantity' and 'new_item\" \n",
    "column and added the values from the various named columns to it\n",
    "- With the new columns in place, we removed all the 'old' columns using etl.cutout\n",
    "- In order to match other tables and to succesffuly join them, we need the columns names the same.\n",
    "    - we did this by converting all cells within the 'store' column, spliting at the _ and replacing the other _ to a space. ('2010-2020_bradford_branch' --> 'bradford branch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_table =etl.addfield(json_table, 'new_quantity', lambda cell: cell['quantity'] + cell ['total_quantity'] + cell ['quantity_purchased'] + cell ['total_quantity_purchased'])\n",
    "# json_table =etl.addfield(json_table, 'new_item', lambda cell: cell['item'] + cell ['product'] + cell ['sku'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_table = etl.cutout(json_table, 'quantity')\n",
    "# json_table = etl.cutout(json_table, 'total_quantity')\n",
    "# json_table = etl.cutout(json_table, 'quantity_purchased')\n",
    "# json_table = etl.cutout(json_table, 'total_quantity_purchased')\n",
    "# json_table = etl.cutout(json_table, 'item')\n",
    "# json_table = etl.cutout(json_table, 'product')\n",
    "# json_table = etl.cutout(json_table, 'sku')\n",
    "\n",
    "# json_table = etl.convert(json_table, 'store', lambda cell: \"_\".join(cell.split('_')[1:]).replace(\"_\", \" \").strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We renamed the 'new_item' & 'new_quantity' -> 'item' & 'quantity' in both tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_table = etl.rename(csv_table, {'new_quantity': 'quantity','new_item':'product', 'store':'branch_name'})\n",
    "# json_table = etl.rename(json_table, {'new_quantity': 'quantity','new_item':'product', 'store':'branch_name'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imported store data from xlsx & product list from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store_data_table = etl.fromxlsx('data/branch_list.xlsx')\n",
    "# product_list = etl.fromcsv('data/products_list.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join the store data table to both the csv and json tables using the 'branch_name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_table_with_store_data = etl.leftjoin(csv_table,store_data_table, key='branch_name')\n",
    "# json_table_with_store_data = etl.leftjoin(json_table,store_data_table, key='branch_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the etl.cat, we concatinated the csv and json tables into 'data_transformation_df'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_transformation_df = etl.cat(csv_table_with_store_data,json_table_with_store_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join the product list table to the  using the 'product' into the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_transformation_df = etl.leftjoin(data_transformation_df,product_list, key='product')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### once joined, we exported into a csv, in order to be able to work on it a bit quicker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# etl.tocsv(data_transformation_df, 'index_output/consumer_behaviour_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We import the csv from the line above and add some final touches\n",
    "- We converted the year, month, day, hour, established_on and amount_in_gbp from a str into a float to be able to continue working on it like an int\n",
    "- we converted quantity into an int\n",
    "- We had two versions of manufactuter, catergory and price (which we recently found was obsolete as contained the wrong data)\n",
    "    - because of this, we used the etl.cutout to remove the manufactuter, catergory and both prices from the table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consumer_behaviour_df = etl.fromcsv('consumer_behaviour_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consumer_behaviour_df= etl.convert(consumer_behaviour_df, 'year', float)\n",
    "# consumer_behaviour_df= etl.convert(consumer_behaviour_df, 'month', float)\n",
    "# consumer_behaviour_df= etl.convert(consumer_behaviour_df, 'day', float)\n",
    "# consumer_behaviour_df= etl.convert(consumer_behaviour_df, 'hour', float)\n",
    "# consumer_behaviour_df= etl.convert(consumer_behaviour_df, 'established_on', float)\n",
    "# consumer_behaviour_df= etl.convert(consumer_behaviour_df, 'amount_in_gbp', float)\n",
    "# consumer_behaviour_df= etl.convert(consumer_behaviour_df, 'quantity', int)\n",
    "\n",
    "\n",
    "\n",
    "# consumer_behaviour_df = etl.cutout(consumer_behaviour_df, 'manufactuter')\n",
    "# consumer_behaviour_df = etl.cutout(consumer_behaviour_df, 'category')\n",
    "# consumer_behaviour_df = etl.cutout(consumer_behaviour_df, 'price')\n",
    "# consumer_behaviour_df = etl.cutout(consumer_behaviour_df, 'price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the .look(), we can check that the table is how we want it before final export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consumer_behaviour_df.look()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We export the final table to csv, ready to be used in the dash app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# etl.tocsv(consumer_behaviour_df, 'index_output/final_consumer_behaviour_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import branch_expenses.xlsx and save as a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branch_expenses = etl.fromxlsx('data/branch_expenses.xlsx')\n",
    "branch_expenses.look()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etl.tocsv(branch_expenses, 'index_output/branch_expenses.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to use the exported files, reduce the file size/ check the code is plotting the right data, I continued my cleaning in './testing_out.ipynb'"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e4baa1118497be939149502021bd6938b195297596de37c7dd3321e4ed94a8c7"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
