{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consumer Behaviour data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/jontylees/Python/final-project/data_transformation/.env/lib/python3.10/site-packages (1.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/jontylees/Python/final-project/data_transformation/.env/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/jontylees/Python/final-project/data_transformation/.env/lib/python3.10/site-packages (from pandas) (1.21.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/jontylees/Python/final-project/data_transformation/.env/lib/python3.10/site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jontylees/Python/final-project/data_transformation/.env/lib/python3.10/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement matplotlib.pyplot (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for matplotlib.pyplot\u001b[0m\n",
      "Requirement already satisfied: plotly.express in /Users/jontylees/Python/final-project/data_transformation/.env/lib/python3.10/site-packages (0.4.1)\n",
      "Requirement already satisfied: scipy>=0.18 in /Users/jontylees/Python/final-project/data_transformation/.env/lib/python3.10/site-packages (from plotly.express) (1.7.3)\n",
      "Requirement already satisfied: pandas>=0.20.0 in /Users/jontylees/Python/final-project/data_transformation/.env/lib/python3.10/site-packages (from plotly.express) (1.3.4)\n",
      "Requirement already satisfied: plotly>=4.1.0 in /Users/jontylees/Python/final-project/data_transformation/.env/lib/python3.10/site-packages (from plotly.express) (5.4.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /Users/jontylees/Python/final-project/data_transformation/.env/lib/python3.10/site-packages (from plotly.express) (1.21.4)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /Users/jontylees/Python/final-project/data_transformation/.env/lib/python3.10/site-packages (from plotly.express) (0.13.1)\n",
      "Requirement already satisfied: patsy>=0.5 in /Users/jontylees/Python/final-project/data_transformation/.env/lib/python3.10/site-packages (from plotly.express) (0.5.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/jontylees/Python/final-project/data_transformation/.env/lib/python3.10/site-packages (from pandas>=0.20.0->plotly.express) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/jontylees/Python/final-project/data_transformation/.env/lib/python3.10/site-packages (from pandas>=0.20.0->plotly.express) (2.8.2)\n",
      "Requirement already satisfied: six in /Users/jontylees/Python/final-project/data_transformation/.env/lib/python3.10/site-packages (from patsy>=0.5->plotly.express) (1.16.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /Users/jontylees/Python/final-project/data_transformation/.env/lib/python3.10/site-packages (from plotly>=4.1.0->plotly.express) (8.0.1)\n",
      "Requirement already satisfied: openpyxl in /Users/jontylees/Python/final-project/data_transformation/.env/lib/python3.10/site-packages (3.0.9)\n",
      "Requirement already satisfied: et-xmlfile in /Users/jontylees/Python/final-project/data_transformation/.env/lib/python3.10/site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: petl in /Users/jontylees/Python/final-project/data_transformation/.env/lib/python3.10/site-packages (1.7.4)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install pandas\n",
    "! pip3 install matplotlib.pyplot\n",
    "! pip3 install plotly.express\n",
    "! pip3 install openpyxl\n",
    "! pip3 install petl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import petl as etl\n",
    "import glob as glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import files from un-zipped folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csv files\n",
    "- Using glob, we extract all the files from the file ending in '.csv'\n",
    "- We use iteration to loop each file:\n",
    "    - export it into frame\n",
    "    - create a new 'store' column, as then alter the basename, removing the '.csv' and inserting file name into 'store' to differentiate which store each row belows too.\n",
    "    - we then append frame to the csv_data empty object\n",
    "- We use concatenation to bring all the files together\n",
    "- Finally exporting to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_data_import = glob.glob('data/*.csv')\n",
    "# csv_data = []\n",
    "# for csv in csv_data_import:\n",
    "#     frame = pd.read_csv(csv)\n",
    "#     frame['store'] = os.path.basename(csv).split(\".\")[0]\n",
    "#     csv_data.append(frame)\n",
    "\n",
    "# csv_final = pd.concat(csv_data, ignore_index=True)\n",
    "\n",
    "# csv_final.to_csv('csv_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### json files\n",
    "- Using glob, we extract all the files from the file ending in '.json'\n",
    "- We use iteration to loop each file:\n",
    "    - export it into frame\n",
    "    - create a new 'store' column, as then alter the basename, removing the '.json' and inserting file name into 'store' to differentiate which store each row belows too.\n",
    "    - we then append frame to the json_data empty object\n",
    "- We use concatenation to bring all the files together\n",
    "- Finally exporting to json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_data_import = glob.glob('data/*.json')\n",
    "# print(json_data_import)\n",
    "\n",
    "# json_data = []\n",
    "# for json in json_data_import:\n",
    "#     frame = pd.read_json(json)\n",
    "#     frame['store'] = os.path.basename(json).split(\".\")[0]\n",
    "#     json_data.append(frame)\n",
    "\n",
    "# json_final = pd.concat(json_data\n",
    "# json_final.to_csv('json_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import csv files into etl tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_table = etl.fromcsv('csv_final.csv')\n",
    "# json_table = etl.fromcsv('json_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csv table\n",
    "- As some of the columns have slightly differnet names but same meaningful data, We use the etl.addfield propety to create a 'new_quantity' and 'new_item\" \n",
    "column and added the values from the various named columns to it\n",
    "- With the new columns in place, we removed all the 'old' columns using etl.cutout\n",
    "- In order to match other tables and to succesffuly join them, we need the columns names the same.\n",
    "    - we did this by converting all cells within the 'store' column, spliting at the _ and replacing the other _ to a space. ('2010-2020_bradford_branch' --> 'bradford branch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_table =etl.addfield(csv_table, 'new_quantity', lambda cell: cell['quantity'] + cell ['total_quantity'] + cell ['quantity_purchased'] + cell ['total_quantity_purchased'])\n",
    "# csv_table =etl.addfield(csv_table, 'new_item', lambda cell: cell['item'] + cell ['product'] + cell ['sku'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# csv_table = etl.cutout(csv_table, 'quantity')\n",
    "# csv_table = etl.cutout(csv_table, 'total_quantity')\n",
    "# csv_table = etl.cutout(csv_table, 'quantity_purchased')\n",
    "# csv_table = etl.cutout(csv_table, 'total_quantity_purchased')\n",
    "# csv_table = etl.cutout(csv_table, 'item')\n",
    "# csv_table = etl.cutout(csv_table, 'product')\n",
    "# csv_table = etl.cutout(csv_table, 'sku')\n",
    "\n",
    "# csv_table = etl.convert(csv_table, 'store', lambda cell: \"_\".join(cell.split('_')[1:]).replace(\"_\", \" \").strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### json table\n",
    "- As some of the columns have slightly differnet names but same meaningful data, We use the etl.addfield propety to create a 'new_quantity' and 'new_item\" \n",
    "column and added the values from the various named columns to it\n",
    "- With the new columns in place, we removed all the 'old' columns using etl.cutout\n",
    "- In order to match other tables and to succesffuly join them, we need the columns names the same.\n",
    "    - we did this by converting all cells within the 'store' column, spliting at the _ and replacing the other _ to a space. ('2010-2020_bradford_branch' --> 'bradford branch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_table =etl.addfield(json_table, 'new_quantity', lambda cell: cell['quantity'] + cell ['total_quantity'] + cell ['quantity_purchased'] + cell ['total_quantity_purchased'])\n",
    "# json_table =etl.addfield(json_table, 'new_item', lambda cell: cell['item'] + cell ['product'] + cell ['sku'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_table = etl.cutout(json_table, 'quantity')\n",
    "# json_table = etl.cutout(json_table, 'total_quantity')\n",
    "# json_table = etl.cutout(json_table, 'quantity_purchased')\n",
    "# json_table = etl.cutout(json_table, 'total_quantity_purchased')\n",
    "# json_table = etl.cutout(json_table, 'item')\n",
    "# json_table = etl.cutout(json_table, 'product')\n",
    "# json_table = etl.cutout(json_table, 'sku')\n",
    "\n",
    "# json_table = etl.convert(json_table, 'store', lambda cell: \"_\".join(cell.split('_')[1:]).replace(\"_\", \" \").strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We renamed the 'new_item' & 'new_quantity' -> 'item' & 'quantity' in both tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_table = etl.rename(csv_table, {'new_quantity': 'quantity','new_item':'product', 'store':'branch_name'})\n",
    "# json_table = etl.rename(json_table, {'new_quantity': 'quantity','new_item':'product', 'store':'branch_name'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imported store data from xlsx & product list from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store_data_table = etl.fromxlsx('data/branch_list.xlsx')\n",
    "# product_list = etl.fromcsv('data/products_list.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join the store data table to both the csv and json tables using the 'branch_name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_table_with_store_data = etl.leftjoin(csv_table,store_data_table, key='branch_name')\n",
    "# json_table_with_store_data = etl.leftjoin(json_table,store_data_table, key='branch_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the etl.cat, we concatinated the csv and json tables into 'data_transformation_df'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_transformation_df = etl.cat(csv_table_with_store_data,json_table_with_store_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join the product list table to the  using the 'product' into the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_transformation_df = etl.leftjoin(data_transformation_df,product_list, key='product')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### once joined, we exported into a csv, in order to be able to work on it a bit quicker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# etl.tocsv(data_transformation_df, 'consumer_behaviour_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We import the csv from the line above and add some final touches\n",
    "- We converted the year, month, day, hour, established_on and amount_in_gbp from a str into a float to be able to continue working on it like an int\n",
    "- we converted quantity into an int\n",
    "- We had two versions of manufactuter, catergory and price (which we recently found was obsolete as contained the wrong data)\n",
    "    - because of this, we used the etl.cutout to remove the manufactuter, catergory and both prices from the table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consumer_behaviour_df = etl.fromcsv('consumer_behaviour_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consumer_behaviour_df= etl.convert(consumer_behaviour_df, 'year', float)\n",
    "# consumer_behaviour_df= etl.convert(consumer_behaviour_df, 'month', float)\n",
    "# consumer_behaviour_df= etl.convert(consumer_behaviour_df, 'day', float)\n",
    "# consumer_behaviour_df= etl.convert(consumer_behaviour_df, 'hour', float)\n",
    "# consumer_behaviour_df= etl.convert(consumer_behaviour_df, 'established_on', float)\n",
    "# consumer_behaviour_df= etl.convert(consumer_behaviour_df, 'amount_in_gbp', float)\n",
    "# consumer_behaviour_df= etl.convert(consumer_behaviour_df, 'quantity', int)\n",
    "\n",
    "\n",
    "\n",
    "# consumer_behaviour_df = etl.cutout(consumer_behaviour_df, 'manufactuter')\n",
    "# consumer_behaviour_df = etl.cutout(consumer_behaviour_df, 'category')\n",
    "# consumer_behaviour_df = etl.cutout(consumer_behaviour_df, 'price')\n",
    "# consumer_behaviour_df = etl.cutout(consumer_behaviour_df, 'price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the .look(), we can check that the table is how we want it before final export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consumer_behaviour_df.look()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We export the final table to csv, ready to be used in the dash app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# etl.tocsv(consumer_behaviour_df, 'final_consumer_behaviour_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_product_df = etl.fromcsv('final_consumer_behaviour_df.csv')\n",
    "branch_expenses =  etl.fromxlsx('data/branch_expenses.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+-------+----------+-------+--------+-------+----------------------+-----------------+----------+------------------------------+--------------------+----------+----------------+---------------+---------------+\n",
       "|       | year     | month | day    | hour  | amount_in_gbp        | branch_name     | quantity | product                      | region             | county   | established_on | manufactuter  | category      |\n",
       "+=======+==========+=======+========+=======+======================+=================+==========+==============================+====================+==========+================+===============+===============+\n",
       "| '166' | '2012.0' | '1.0' | '10.0' | '0.0' | '367.50000000000006' | 'Armagh outlet' | '125'    | '21 day mature ribeye steak' | 'Northern Ireland' | 'Armagh' | '2012.0'       | 'farmer jack' | 'meat & fish' |\n",
       "+-------+----------+-------+--------+-------+----------------------+-----------------+----------+------------------------------+--------------------+----------+----------------+---------------+---------------+\n",
       "| '370' | '2012.0' | '1.0' | '10.0' | '0.0' | '44.10000000000001'  | 'Armagh outlet' | '15'     | '21 day mature ribeye steak' | 'Northern Ireland' | 'Armagh' | '2012.0'       | 'farmer jack' | 'meat & fish' |\n",
       "+-------+----------+-------+--------+-------+----------------------+-----------------+----------+------------------------------+--------------------+----------+----------------+---------------+---------------+\n",
       "| '411' | '2012.0' | '1.0' | '10.0' | '0.0' | '311.64000000000004' | 'Armagh outlet' | '106'    | '21 day mature ribeye steak' | 'Northern Ireland' | 'Armagh' | '2012.0'       | 'farmer jack' | 'meat & fish' |\n",
       "+-------+----------+-------+--------+-------+----------------------+-----------------+----------+------------------------------+--------------------+----------+----------------+---------------+---------------+\n",
       "| '553' | '2012.0' | '1.0' | '10.0' | '0.0' | '149.94000000000003' | 'Armagh outlet' | '51'     | '21 day mature ribeye steak' | 'Northern Ireland' | 'Armagh' | '2012.0'       | 'farmer jack' | 'meat & fish' |\n",
       "+-------+----------+-------+--------+-------+----------------------+-----------------+----------+------------------------------+--------------------+----------+----------------+---------------+---------------+\n",
       "| '904' | '2012.0' | '1.0' | '16.0' | '1.0' | '308.70000000000005' | 'Armagh outlet' | '105'    | '21 day mature ribeye steak' | 'Northern Ireland' | 'Armagh' | '2012.0'       | 'farmer jack' | 'meat & fish' |\n",
       "+-------+----------+-------+--------+-------+----------------------+-----------------+----------+------------------------------+--------------------+----------+----------------+---------------+---------------+\n",
       "..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_product_df.look()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+--------+-------+-------------------+----------------+----------------------+----------------+------------------+---------------+---------------+------------+\n",
       "| year   | month | region            | county         | branch_name          | established_on | operational_cost | staff_bonuses | misc_expenses | waste_cost |\n",
       "+========+=======+===================+================+======================+================+==================+===============+===============+============+\n",
       "| 2012.0 |   1.0 | 'East of England' | 'Bedfordshire' | 'Bedfordshire store' |         2012.0 |           2785.0 |         232.0 |        7793.0 |     4729.0 |\n",
       "+--------+-------+-------------------+----------------+----------------------+----------------+------------------+---------------+---------------+------------+\n",
       "| 2012.0 |   2.0 | 'East of England' | 'Bedfordshire' | 'Bedfordshire store' |         2012.0 |           2058.0 |         260.0 |        3559.0 |     3539.0 |\n",
       "+--------+-------+-------------------+----------------+----------------------+----------------+------------------+---------------+---------------+------------+\n",
       "| 2012.0 |   3.0 | 'East of England' | 'Bedfordshire' | 'Bedfordshire store' |         2012.0 |           2222.0 |         319.0 |        7320.0 |     2809.0 |\n",
       "+--------+-------+-------------------+----------------+----------------------+----------------+------------------+---------------+---------------+------------+\n",
       "| 2012.0 |   4.0 | 'East of England' | 'Bedfordshire' | 'Bedfordshire store' |         2012.0 |           2324.0 |         271.0 |        3352.0 |     3704.0 |\n",
       "+--------+-------+-------------------+----------------+----------------------+----------------+------------------+---------------+---------------+------------+\n",
       "| 2012.0 |   5.0 | 'East of England' | 'Bedfordshire' | 'Bedfordshire store' |         2012.0 |           2489.0 |         263.0 |        5834.0 |     3872.0 |\n",
       "+--------+-------+-------------------+----------------+----------------------+----------------+------------------+---------------+---------------+------------+\n",
       "..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "branch_expenses.look()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+--------+-------+-------------------+----------------+----------------------+----------------+------------------+---------------+---------------+------------+\n",
       "| year   | month | region            | county         | branch_name          | established_on | operational_cost | staff_bonuses | misc_expenses | waste_cost |\n",
       "+========+=======+===================+================+======================+================+==================+===============+===============+============+\n",
       "| 2012.0 |   1.0 | 'East of England' | 'Bedfordshire' | 'Bedfordshire store' |         2012.0 |           2785.0 |         232.0 |        7793.0 |     4729.0 |\n",
       "+--------+-------+-------------------+----------------+----------------------+----------------+------------------+---------------+---------------+------------+\n",
       "| 2012.0 |   2.0 | 'East of England' | 'Bedfordshire' | 'Bedfordshire store' |         2012.0 |           2058.0 |         260.0 |        3559.0 |     3539.0 |\n",
       "+--------+-------+-------------------+----------------+----------------------+----------------+------------------+---------------+---------------+------------+\n",
       "| 2012.0 |   3.0 | 'East of England' | 'Bedfordshire' | 'Bedfordshire store' |         2012.0 |           2222.0 |         319.0 |        7320.0 |     2809.0 |\n",
       "+--------+-------+-------------------+----------------+----------------------+----------------+------------------+---------------+---------------+------------+\n",
       "| 2012.0 |   4.0 | 'East of England' | 'Bedfordshire' | 'Bedfordshire store' |         2012.0 |           2324.0 |         271.0 |        3352.0 |     3704.0 |\n",
       "+--------+-------+-------------------+----------------+----------------------+----------------+------------------+---------------+---------------+------------+\n",
       "| 2012.0 |   5.0 | 'East of England' | 'Bedfordshire' | 'Bedfordshire store' |         2012.0 |           2489.0 |         263.0 |        5834.0 |     3872.0 |\n",
       "+--------+-------+-------------------+----------------+----------------------+----------------+------------------+---------------+---------------+------------+\n",
       "..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "branch_expenses = etl.fromxlsx('data/branch_expenses.xlsx')\n",
    "branch_expenses.look()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "etl.tocsv(branch_expenses, 'branch_expenses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e4baa1118497be939149502021bd6938b195297596de37c7dd3321e4ed94a8c7"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
